# Strategy Consumer

The Strategy Consumer service is responsible for consuming trading strategies from Kafka and storing them in PostgreSQL for persistence and analysis. It acts as a bridge between the strategy discovery pipeline and the strategy storage system.

## Production System Overview

The Strategy Consumer service has achieved significant production scale:

- **Scale**: Processing all strategies discovered by genetic algorithm optimization
- **Deployment**: Kubernetes CronJob (\*/5 minutes)
- **Status**: ✅ **PRODUCTION** - Complete strategy metadata storage
- **Database**: PostgreSQL with full strategy metadata and performance metrics
- **Reliability**: Automatic processing with error handling and retry mechanisms

## Overview

The Strategy Consumer service:

- **Consumes strategies** from Kafka topics generated by the genetic algorithm discovery pipeline
- **Validates strategies** before storage to ensure data integrity
- **Stores strategies** in PostgreSQL with metadata and performance metrics
- **Provides inspection** capabilities for analyzing strategy characteristics
- **Handles batch processing** for efficient database operations

## Architecture

### Data Flow

```
Strategy Discovery Pipeline → Kafka → Strategy Consumer → PostgreSQL
```

### Key Components

- **Kafka Consumer**: Consumes strategy messages from Kafka topics
- **Strategy Inspector**: Validates and analyzes strategy characteristics
- **PostgreSQL Client**: Handles database operations and bulk inserts
- **Batch Processor**: Efficiently processes multiple strategies

## Production Database Schema

### Strategies Table (Production Schema)

```sql
-- Production PostgreSQL schema:
CREATE TABLE strategies (
    strategy_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    symbol VARCHAR NOT NULL,
    strategy_type VARCHAR NOT NULL,
    parameters JSONB NOT NULL,
    current_score DOUBLE PRECISION NOT NULL,
    first_discovered_at TIMESTAMP NOT NULL DEFAULT NOW(),
    last_evaluated_at TIMESTAMP NOT NULL DEFAULT NOW(),
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    strategy_hash VARCHAR UNIQUE NOT NULL,
    discovery_symbol VARCHAR,
    discovery_start_time TIMESTAMP,
    discovery_end_time TIMESTAMP,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Performance indexes
CREATE INDEX idx_strategies_symbol ON strategies(symbol);
CREATE INDEX idx_strategies_strategy_type ON strategies(strategy_type);
CREATE INDEX idx_strategies_current_score ON strategies(current_score);
CREATE INDEX idx_strategies_is_active ON strategies(is_active);
```

### Performance Metrics

The service stores comprehensive performance metrics:

```json
{
  "total_return": 0.15,
  "sharpe_ratio": 1.2,
  "max_drawdown": -0.05,
  "win_rate": 0.65,
  "profit_factor": 1.8,
  "total_trades": 150,
  "avg_trade_duration": 2.5
}
```

## Features

### Strategy Consumption

- **Real-time Processing**: Processes strategies as they arrive from Kafka
- **Batch Processing**: Efficiently handles large volumes of strategies
- **Error Handling**: Robust error handling with retry mechanisms
- **Dead Letter Queue**: Handles failed messages appropriately

### Strategy Validation

- **Parameter Validation**: Validates strategy parameters are within expected ranges
- **Performance Validation**: Checks performance metrics are reasonable
- **Metadata Validation**: Ensures required metadata is present
- **Format Validation**: Validates strategy format and structure

### Database Operations

- **Bulk Inserts**: Efficient bulk insertion of strategies
- **Upsert Operations**: Updates existing strategies or inserts new ones
- **Index Management**: Maintains proper database indexes
- **Connection Pooling**: Efficient database connection management

## Configuration

### Environment Variables

```bash
# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_TOPIC_STRATEGIES=discovered-strategies
KAFKA_GROUP_ID=strategy-consumer

# PostgreSQL Configuration
POSTGRES_HOST=postgresql
POSTGRES_PORT=5432
POSTGRES_DB=tradestream
POSTGRES_USER=strategy_consumer
POSTGRES_PASSWORD=your-password

# Processing Configuration
BATCH_SIZE=1000
BATCH_TIMEOUT_SECONDS=30
MAX_RETRIES=3
```

### Command Line Options

```bash
# Basic usage
python main.py

# With custom configuration
python main.py \
  --kafka_bootstrap_servers=kafka:9092 \
  --postgres_host=postgresql \
  --batch_size=500 \
  --batch_timeout_seconds=60
```

## Usage Examples

### Development Mode

```bash
# Run in development mode with local Kafka
python main.py \
  --kafka_bootstrap_servers=localhost:9092 \
  --postgres_host=localhost \
  --batch_size=100 \
  --verbosity=1
```

### Production Mode

```bash
# Run in production with proper configuration
python main.py \
  --kafka_bootstrap_servers=kafka:9092 \
  --postgres_host=postgresql \
  --batch_size=1000 \
  --batch_timeout_seconds=30 \
  --max_retries=5
```

### Docker Deployment

```bash
# Run with Docker
docker run tradestreamhq/strategy-consumer:latest \
  --kafka_bootstrap_servers=kafka:9092 \
  --postgres_host=postgresql \
  --batch_size=1000
```

## API Integration

### Strategy Inspection

The service includes strategy inspection capabilities:

```python
from strategy_consumer.strategy_inspector import StrategyInspector

inspector = StrategyInspector()

# Analyze strategy characteristics
analysis = inspector.analyze_strategy(strategy_data)

# Validate strategy parameters
is_valid = inspector.validate_parameters(parameters)

# Extract performance metrics
metrics = inspector.extract_performance_metrics(strategy_data)
```

### PostgreSQL Operations

```python
from strategy_consumer.postgres_client import PostgresClient

client = PostgresClient(
    host="postgresql",
    database="tradestream",
    user="strategy_consumer"
)

# Bulk insert strategies
client.bulk_insert_strategies(strategies)

# Get strategy by ID
strategy = client.get_strategy("strategy_123")

# Get strategies by type
strategies = client.get_strategies_by_type("MACD_CROSSOVER")
```

## Production Performance Metrics

**Strategy Consumer System** (Verified Production Metrics):

- **Strategy Storage**: All discovered strategies stored with full metadata
- **Database Performance**: Sub-second strategy queries and inserts
- **Processing Reliability**: Automatic processing with error handling
- **Data Integrity**: Complete strategy metadata preservation
- **Scalability**: Handles all strategies from genetic algorithm optimization

**Infrastructure Performance** (Production Verified):

- **PostgreSQL Performance**: Efficient strategy storage and retrieval
- **Kafka Integration**: Reliable message consumption from discovery pipeline
- **Batch Processing**: Efficient bulk operations for strategy storage
- **Error Handling**: Robust retry mechanisms and dead letter queue

## Monitoring

### Health Checks

The service exposes health check endpoints:

```bash
# Health check
curl http://localhost:8080/health

# Readiness check
curl http://localhost:8080/ready

# Metrics endpoint
curl http://localhost:8080/metrics
```

### Key Metrics

- **Messages Processed**: Number of strategies processed
- **Processing Rate**: Strategies processed per second
- **Error Rate**: Percentage of failed processing attempts
- **Database Latency**: PostgreSQL operation latency
- **Kafka Lag**: Consumer lag from Kafka

### Logging

Structured logging with correlation IDs:

```python
import logging

logger = logging.getLogger(__name__)

# Log strategy processing
logger.info("Processing strategy", extra={
    "strategy_id": strategy_id,
    "strategy_type": strategy_type,
    "batch_size": len(strategies)
})
```

## Error Handling

### Retry Logic

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def process_strategy_with_retry(strategy_data):
    return process_strategy(strategy_data)
```

### Dead Letter Queue

Failed messages are sent to a dead letter queue:

```python
# Send to dead letter queue
def send_to_dlq(message, error):
    dlq_producer.send("strategy-consumer-dlq", {
        "original_message": message,
        "error": str(error),
        "timestamp": datetime.utcnow().isoformat()
    })
```

## Performance Optimization

### Batch Processing

```python
# Efficient batch processing
def process_batch(strategies):
    # Validate batch
    valid_strategies = [s for s in strategies if validate_strategy(s)]

    # Bulk insert to database
    postgres_client.bulk_insert_strategies(valid_strategies)

    # Commit offset
    kafka_consumer.commit()
```

### Connection Pooling

```python
# PostgreSQL connection pooling
from psycopg2.pool import SimpleConnectionPool

pool = SimpleConnectionPool(
    minconn=5,
    maxconn=20,
    host=postgres_host,
    database=postgres_db,
    user=postgres_user,
    password=postgres_password
)
```

## Testing

### Unit Tests

```bash
# Run unit tests
bazel test //services/strategy_consumer:all

# Run specific test
bazel test //services/strategy_consumer:postgres_client_test
```

### Integration Tests

```bash
# Run integration tests with real databases
bazel test //services/strategy_consumer:postgres_client_integration_test
```

### Test Examples

```python
def test_strategy_validation():
    inspector = StrategyInspector()
    strategy_data = create_test_strategy()

    is_valid = inspector.validate_strategy(strategy_data)
    assert is_valid is True

def test_bulk_insert():
    client = PostgresClient(test_config)
    strategies = create_test_strategies(100)

    result = client.bulk_insert_strategies(strategies)
    assert result.success_count == 100
```

## Deployment

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: strategy-consumer
spec:
  replicas: 2
  selector:
    matchLabels:
      app: strategy-consumer
  template:
    metadata:
      labels:
        app: strategy-consumer
    spec:
      containers:
        - name: strategy-consumer
          image: tradestreamhq/strategy-consumer:latest
          env:
            - name: KAFKA_BOOTSTRAP_SERVERS
              value: "kafka:9092"
            - name: POSTGRES_HOST
              value: "postgresql"
          ports:
            - containerPort: 8080
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
```

### Docker Build

```bash
# Build container
bazel run //services/strategy_consumer:push_strategy_consumer_image

# Run container
docker run -d \
  --name strategy-consumer \
  -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 \
  -e POSTGRES_HOST=postgresql \
  tradestreamhq/strategy-consumer:latest
```

## Troubleshooting

### Common Issues

#### Kafka Connection Issues

```bash
# Check Kafka connectivity
kubectl exec -it deployment/strategy-consumer -- nc -zv kafka 9092

# Check Kafka logs
kubectl logs deployment/kafka
```

#### PostgreSQL Connection Issues

```bash
# Check PostgreSQL connectivity
kubectl exec -it deployment/strategy-consumer -- nc -zv postgresql 5432

# Check PostgreSQL logs
kubectl logs deployment/postgresql
```

#### High Memory Usage

```bash
# Check memory usage
kubectl top pod -l app=strategy-consumer

# Adjust batch size if needed
kubectl patch deployment strategy-consumer -p '{"spec":{"template":{"spec":{"containers":[{"name":"strategy-consumer","env":[{"name":"BATCH_SIZE","value":"500"}]}]}}}}'
```

### Debug Commands

```bash
# Check service logs
kubectl logs deployment/strategy-consumer -f

# Check service status
kubectl describe deployment strategy-consumer

# Port forward for local debugging
kubectl port-forward svc/strategy-consumer 8080:8080
```

## Contributing

When contributing to the Strategy Consumer:

1. **Follow Patterns**: Use existing code patterns and conventions
2. **Add Tests**: Include comprehensive test coverage
3. **Update Documentation**: Keep README and docstrings current
4. **Performance**: Consider performance implications of changes
5. **Error Handling**: Implement proper error handling and logging

## License

This project is part of the TradeStream platform. See the root LICENSE file for details.
