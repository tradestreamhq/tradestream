influxdb:
  enabled: true
  # Authentication configuration
  adminUser:
    # Organization and bucket settings
    organization: "tradestream-org"
    bucket: "tradestream-data"
    retention_policy: "30d"
    # User credentials - these should be strings, not objects
    # username: "admin"       # Use username, not user
    # password: "your-secure-password"  # Direct string, not nested

    # Or use existing secret (comment out username/password if using this)
    existingSecret: "influxdb-admin-secret"
    # Token configuration - should be a string or boolean
    token: true # Set to true to auto-generate, or provide a specific token string
  # Storage configuration
  persistence:
    enabled: true
    size: 8Gi
  service:
    type: ClusterIP
kafka:
  replicaCount: 3
  persistence:
    enabled: true
    size: 10Gi
    accessModes:
      - ReadWriteOnce
  listeners:
    client:
      protocol: PLAINTEXT
    controller:
      protocol: PLAINTEXT
  configurationOverrides:
    "controller.quorum.voters": "0@kafka-0.kafka-headless:9093,1@kafka-1.kafka-headless:9093,2@kafka-2.kafka-headless:9093"
  podSecurityContext:
    runAsUser: 1001
    fsGroup: 1001
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"
kafkaUi:
  replicaCount: 1
  image:
    repository: provectuslabs/kafka-ui
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8080
pipeline:
  runMode: wet
  image:
    repository: tradestreamhq/tradestream-data-pipeline
    tag: v1.29.4-develop
    pullPolicy: IfNotPresent
  version: v1_18
  configuration:
    env.java.opts.jobmanager: "-Dio.jenetics.util.defaultRandomGenerator=Random"
    env.java.opts.taskmanager: "-Dio.jenetics.util.defaultRandomGenerator=Random"
    taskmanager.numberOfTaskSlots: "1"
    execution.checkpointing.interval: "300000"
    execution.checkpointing.mode: "EXACTLY_ONCE"
    execution.checkpointing.timeout: "1800000"
    execution.checkpointing.externalized-checkpoint-retention: "RETAIN_ON_CANCELLATION"
    state.backend: "rocksdb"
    state.backend.incremental: "true"
  serviceAccount: flink
  jobManager:
    memory: 4096m
    cpu: 2
  taskManager:
    memory: 4096m
    cpu: 2
  job:
    entryClass: com.verlumen.tradestream.pipeline.App
    jarURI: local:///src/main/java/com/verlumen/tradestream/pipeline/app_deploy.jar
    parallelism: 1
    upgradeMode: stateless

# -- Candle Ingestor Configuration --
candleIngestor:
  enabled: false
  replicaCount: 1
  image:
    repository: "your-docker-registry/candle-ingestor" # Placeholder, updated by CI/CD
    tag: "latest" # Placeholder, updated by CI/CD
    pullPolicy: IfNotPresent
  # Environment variables for the candle ingestor script
  # These are accessed by the Python script via absl.flags
  # The deployment maps .Values.candleIngestor.env.KEY to an env var named FLAGS_KEY or a direct name
  # Corrected keys to uppercase with underscores as per new instructions.
  # Note: The candle-ingestor-deployment.yaml template will need to be updated to reference these new keys
  # (e.g., .Values.candleIngestor.env.INFLUXDB_URL instead of .Values.candleIngestor.env.influxDbUrl).
  env:
    INFLUXDB_URL: "http://influxdb.tradestream-namespace.svc.cluster.local:8086" # Default, adjust if InfluxDB service name differs
    INFLUXDB_BUCKET: "tradestream-data" # Should match the bucket created by InfluxDB setup
    TOP_N_CRYPTOS: "20"
    CANDLE_GRANULARITY_MINUTES: "1"
    BACKFILL_START_DATE: "1_year_ago"
    POLLING_INITIAL_CATCHUP_DAYS: "7"
    TIINGO_API_CALL_DELAY_SECONDS: "2"

  # Names of the Kubernetes secrets to source API keys and InfluxDB token/org from
  envFromSecrets:
    cmcApiKeySecretRefName: "coinmarketcap" # Assumed existing secret for CoinMarketCap API key
    tiingoApiKeySecretRefName: "tiingo"     # Assumed existing secret for Tiingo API key
    influxDbSecretRefName: "influxdb-candle-ingestor-credentials" # NEW secret for this app's InfluxDB token & org

  # Resource requests and limits for the candle ingestor pod
  # Example:
  # resources:
  #   requests:
  #     cpu: "100m"
  #     memory: "256Mi"
  #   limits:
  #     cpu: "500m"
  #     memory: "512Mi"
  resources: {}

  # Node selector, tolerations, and affinity for pod scheduling
  nodeSelector: {}
  tolerations: []
  affinity: {}
